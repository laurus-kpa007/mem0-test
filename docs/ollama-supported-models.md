# Ollama ê³µì‹ ì§€ì› ëª¨ë¸ ë¦¬ìŠ¤íŠ¸

## 1. âœ… Ollama ê³µì‹ ì§€ì› ëª¨ë¸ (2024ë…„ 11ì›” ê¸°ì¤€)

### 1.1 ëŒ€í™”í˜• ëª¨ë¸ (Chat Models)

#### âœ… **Qwen ì‹œë¦¬ì¦ˆ** (Alibaba)
```bash
# ê³µì‹ ì§€ì›
ollama pull qwen2.5:0.5b   # 0.5B íŒŒë¼ë¯¸í„°
ollama pull qwen2.5:1.5b   # 1.5B íŒŒë¼ë¯¸í„°
ollama pull qwen2.5:3b     # 3B íŒŒë¼ë¯¸í„°
ollama pull qwen2.5:7b     # 7B íŒŒë¼ë¯¸í„° â­ ì¶”ì²œ
ollama pull qwen2.5:14b    # 14B íŒŒë¼ë¯¸í„°
ollama pull qwen2.5:32b    # 32B íŒŒë¼ë¯¸í„°
ollama pull qwen2.5:72b    # 72B íŒŒë¼ë¯¸í„°

# Coder ë²„ì „ (ì½”ë“œ íŠ¹í™”)
ollama pull qwen2.5-coder:7b
```

#### âœ… **Llama ì‹œë¦¬ì¦ˆ** (Meta)
```bash
# Llama 3.2
ollama pull llama3.2:1b    # 1B íŒŒë¼ë¯¸í„°
ollama pull llama3.2:3b    # 3B íŒŒë¼ë¯¸í„° â­ ì¶”ì²œ
ollama pull llama3.2:8b    # 8B íŒŒë¼ë¯¸í„°

# Llama 3.1
ollama pull llama3.1:8b
ollama pull llama3.1:70b
ollama pull llama3.1:405b
```

#### âœ… **Mistral ì‹œë¦¬ì¦ˆ**
```bash
ollama pull mistral:7b          # Mistral 7B
ollama pull mistral-nemo:12b    # Mistral Nemo 12B
ollama pull mistral-small:22b   # Mistral Small 22B
```

#### âœ… **Gemma ì‹œë¦¬ì¦ˆ** (Google)
```bash
ollama pull gemma2:2b
ollama pull gemma2:9b
ollama pull gemma2:27b
```

#### âœ… **Phi ì‹œë¦¬ì¦ˆ** (Microsoft)
```bash
ollama pull phi3:3.8b
ollama pull phi3:14b
ollama pull phi3.5:3.8b
```

### 1.2 ìž„ë² ë”© ëª¨ë¸ (Embedding Models)

#### âœ… **ê³µì‹ ì§€ì› ìž„ë² ë”© ëª¨ë¸**
```bash
ollama pull nomic-embed-text    # Nomic AI ìž„ë² ë”© (137M)
ollama pull mxbai-embed-large   # MixedBread AI (335M)
ollama pull all-minilm          # Sentence Transformers (23M)
ollama pull bge-small           # BAAI BGE Small (33M)
ollama pull bge-base            # BAAI BGE Base (109M)
ollama pull bge-large           # BAAI BGE Large (335M)
```

#### âš ï¸ **BGE-M3ëŠ” ì§ì ‘ ë³€í™˜ í•„ìš”**
```bash
# BGE-M3ëŠ” Ollama ê³µì‹ ì§€ì› X
# í•˜ì§€ë§Œ ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ì¶”ê°€ ê°€ëŠ¥ (ì•„ëž˜ ì°¸ì¡°)
```

### 1.3 í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸

#### âš ï¸ **ë¶€ë¶„ ì§€ì› ë˜ëŠ” ì»¤ë®¤ë‹ˆí‹° ëª¨ë¸**
```bash
# Solar - ì»¤ë®¤ë‹ˆí‹° ë²„ì „ ì¡´ìž¬
ollama pull solar:10.7b  # í™•ì¸ í•„ìš”

# EEVE-Korean - ê³µì‹ ì§€ì› X
# ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ì¶”ê°€ í•„ìš”

# Polyglot-Ko - ê³µì‹ ì§€ì› X
# ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ì¶”ê°€ í•„ìš”
```

## 2. ðŸ”§ ì»¤ìŠ¤í…€ ëª¨ë¸ ì¶”ê°€ ë°©ë²•

### 2.1 GGUF íŒŒì¼ë¡œ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

```bash
# 1. GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Hugging Faceì—ì„œ)
wget https://huggingface.co/username/model/resolve/main/model.gguf

# 2. Modelfile ìƒì„±
cat > Modelfile << EOF
FROM ./model.gguf

TEMPLATE """{{ .Prompt }}"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "</s>"
EOF

# 3. Ollama ëª¨ë¸ ìƒì„±
ollama create mymodel -f Modelfile

# 4. ì‹¤í–‰
ollama run mymodel
```

### 2.2 BGE-M3 ì»¤ìŠ¤í…€ ì¶”ê°€ ì˜ˆì‹œ

```bash
# BGE-M3ë¥¼ Ollamaì— ì¶”ê°€í•˜ëŠ” ë°©ë²•
cat > Modelfile-bge-m3 << 'EOF'
FROM ./bge-m3.gguf

TEMPLATE """{{ .Prompt }}"""

PARAMETER embedding_only true
PARAMETER pooling_type mean
EOF

ollama create bge-m3 -f Modelfile-bge-m3
```

## 3. ðŸ“Š í”„ë¡œì íŠ¸ìš© ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸

### 3.1 âœ… **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ (ê³µì‹ ì§€ì›)**

```yaml
ëŒ€í™” ëª¨ë¸:
  - qwen2.5:7b â­â­â­â­â­ (ìµœê³  ì¶”ì²œ)
  - qwen2.5:3b â­â­â­â­ (ê²½ëŸ‰)
  - llama3.2:3b â­â­â­ (ë°±ì—…)
  - llama3.2:8b â­â­â­
  - mistral:7b â­â­
  - gemma2:9b â­â­â­
  - phi3.5:3.8b â­â­ (ê²½ëŸ‰)

ìž„ë² ë”© ëª¨ë¸:
  - nomic-embed-text â­â­â­â­ (ì¶”ì²œ)
  - mxbai-embed-large â­â­â­â­ (ëŒ€ì•ˆ)
  - bge-large â­â­â­
  - all-minilm â­â­ (ê²½ëŸ‰)
```

### 3.2 âš ï¸ **ì¶”ê°€ ìž‘ì—… í•„ìš”**

```yaml
ì»¤ìŠ¤í…€ ë³€í™˜ í•„ìš”:
  - BGE-M3 (GGUF ë³€í™˜ í•„ìš”)
  - Solar-10.7B (ì»¤ë®¤ë‹ˆí‹° ë²„ì „ í™•ì¸)
  - EEVE-Korean (GGUF ë³€í™˜ í•„ìš”)
```

## 4. ðŸŽ¯ ìˆ˜ì •ëœ ìµœì¢… ì¶”ì²œ

### 4.1 **ë©”ì¸ êµ¬ì„± (ëª¨ë‘ Ollama ê³µì‹ ì§€ì›)**

```python
# config.py
OLLAMA_MODELS = {
    "chat": "qwen2.5:7b",           # ë©”ì¸ ëŒ€í™” (ê³µì‹ ì§€ì›) âœ…
    "light": "qwen2.5:3b",           # ê²½ëŸ‰ ìž‘ì—… (ê³µì‹ ì§€ì›) âœ…
    "embedding": "nomic-embed-text",  # ìž„ë² ë”© (ê³µì‹ ì§€ì›) âœ…
    "fallback": "llama3.2:3b"        # ë°±ì—… (ê³µì‹ ì§€ì›) âœ…
}

# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
install_models.sh:
"""
#!/bin/bash
# ëª¨ë“  ëª¨ë¸ Ollama ê³µì‹ ì§€ì›

ollama pull qwen2.5:7b
ollama pull qwen2.5:3b
ollama pull nomic-embed-text
ollama pull llama3.2:3b

echo "âœ… ëª¨ë“  ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ"
ollama list
"""
```

### 4.2 **ëŒ€ì•ˆ êµ¬ì„± (í•œêµ­ì–´ ì¤‘ì‹¬)**

```python
# í•œêµ­ì–´ ìµœì í™” êµ¬ì„±
OLLAMA_MODELS_KR = {
    "chat": "qwen2.5:14b",           # ë” í° ëª¨ë¸ë¡œ í•œêµ­ì–´ ì„±ëŠ¥ í–¥ìƒ âœ…
    "light": "gemma2:2b",            # Google ê²½ëŸ‰ ëª¨ë¸ âœ…
    "embedding": "mxbai-embed-large", # ë‹¤êµ­ì–´ ìž„ë² ë”© âœ…
    "fallback": "phi3.5:3.8b"        # MS ê²½ëŸ‰ ëª¨ë¸ âœ…
}
```

## 5. ðŸ“ ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

| ëª¨ë¸ | ì–‘ìží™” | VRAM/RAM | ê³µì‹ ì§€ì› |
|------|--------|----------|-----------|
| qwen2.5:7b | Q4_K_M | 4-5GB | âœ… |
| qwen2.5:3b | Q4_K_M | 2-3GB | âœ… |
| qwen2.5:14b | Q4_K_M | 8-9GB | âœ… |
| llama3.2:3b | Q4_K_M | 2-3GB | âœ… |
| llama3.2:8b | Q4_K_M | 5-6GB | âœ… |
| mistral:7b | Q4_K_M | 4-5GB | âœ… |
| gemma2:9b | Q4_K_M | 5-6GB | âœ… |
| nomic-embed-text | F16 | 274MB | âœ… |
| mxbai-embed-large | F16 | 670MB | âœ… |

## 6. ðŸš€ ë¹ ë¥¸ ì‹œìž‘ ê°€ì´ë“œ

```bash
# 1. Ollama ì„¤ì¹˜ (Windows)
# https://ollama.com/download/windows ì—ì„œ ë‹¤ìš´ë¡œë“œ

# 2. í•„ìˆ˜ ëª¨ë¸ ì„¤ì¹˜ (ëª¨ë‘ ê³µì‹ ì§€ì›)
ollama pull qwen2.5:7b
ollama pull nomic-embed-text

# 3. í…ŒìŠ¤íŠ¸
ollama run qwen2.5:7b "ì•ˆë…•í•˜ì„¸ìš”, í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ìž…ë‹ˆë‹¤."

# 4. Pythonì—ì„œ ì‚¬ìš©
pip install ollama

python -c "
import ollama
response = ollama.chat(model='qwen2.5:7b', messages=[
    {'role': 'user', 'content': 'ì•ˆë…•í•˜ì„¸ìš”!'}
])
print(response['message']['content'])
"
```

## 7. âš ï¸ ì£¼ì˜ì‚¬í•­

### ê³µì‹ ì§€ì› ëª¨ë¸ í™•ì¸ ë°©ë²•
```bash
# Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
ollama list

# Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê²€ìƒ‰
ollama search qwen
ollama search llama
ollama search embed
```

### ëª¨ë¸ ì—…ë°ì´íŠ¸
```bash
# ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
ollama pull qwen2.5:7b

# íŠ¹ì • ë²„ì „ ê³ ì •
ollama pull qwen2.5:7b-q4_K_M
```

## 8. ê²°ë¡ 

âœ… **í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ëª¨ë¸ (ëª¨ë‘ Ollama ê³µì‹ ì§€ì›)**:
- **ëŒ€í™”**: `qwen2.5:7b` (í•œêµ­ì–´ ìš°ìˆ˜, ê³µì‹ ì§€ì›)
- **ë¶„ë¥˜**: `qwen2.5:3b` (ë¹ ë¥¸ ì²˜ë¦¬, ê³µì‹ ì§€ì›)
- **ìž„ë² ë”©**: `nomic-embed-text` (íš¨ìœ¨ì , ê³µì‹ ì§€ì›)
- **ë°±ì—…**: `llama3.2:3b` (ê²½ëŸ‰, ê³µì‹ ì§€ì›)

ì´ êµ¬ì„±ì€ ëª¨ë‘ Ollamaì—ì„œ ê³µì‹ ì§€ì›í•˜ë¯€ë¡œ ë³„ë„ì˜ ë³€í™˜ ìž‘ì—… ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!