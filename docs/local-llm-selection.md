# ë¡œì»¬ LLM ì„ íƒ ê°€ì´ë“œ

## 1. í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ ë¶„ì„

### 1.1 LLMì´ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—…
- **ëŒ€í™” ì‘ë‹µ ìƒì„±**: ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”
- **ë©”ëª¨ë¦¬ ì¶”ì¶œ**: ëŒ€í™”ì—ì„œ ì¤‘ìš” ì •ë³´ ì‹ë³„
- **í…ìŠ¤íŠ¸ ë¶„ë¥˜**: ë©”ëª¨ë¦¬ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
- **ìš”ì•½**: ê¸´ ëŒ€í™” ë‚´ìš© ìš”ì•½
- **ì„ë² ë”© ìƒì„±**: ë²¡í„° ê²€ìƒ‰ìš© ì„ë² ë”©
- **ì§ˆë¬¸ ì´í•´**: ì‚¬ìš©ì ì˜ë„ íŒŒì•…

### 1.2 ê¸°ìˆ ì  ì œì•½ì‚¬í•­
- **í•˜ë“œì›¨ì–´**: ì¼ë°˜ ê°œì¸ PC/ì„œë²„ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- **ì‘ë‹µ ì†ë„**: 3ì´ˆ ì´ë‚´ ì‘ë‹µ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: RAM 16GB ì´ë‚´
- **GPU**: ì„ íƒì  (CPUë¡œë„ ì‹¤í–‰ ê°€ëŠ¥)
- **í•œêµ­ì–´ ì§€ì›**: í•„ìˆ˜

## 2. ì¶”ì²œ LLM ëª¨ë¸

### 2.1 ğŸ¥‡ 1ìˆœìœ„: Qwen2.5 ì‹œë¦¬ì¦ˆ

#### Qwen2.5-7B-Instruct
```yaml
ëª¨ë¸ í¬ê¸°: 7B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 4-6GB (4bit ì–‘ìí™”)
ì†ë„: ë¹ ë¦„ (RTX 3060ì—ì„œ 30-50 tokens/s)
í•œêµ­ì–´: ìš°ìˆ˜
íŠ¹ì§•:
  - ìµœì‹  ëª¨ë¸ (2024ë…„ ë¦´ë¦¬ì¦ˆ)
  - ë›°ì–´ë‚œ í•œêµ­ì–´ ì´í•´ë ¥
  - ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì› (32K tokens)
  - êµ¬ì¡°í™”ëœ ì¶œë ¥ ìš°ìˆ˜
```

#### Qwen2.5-14B-Instruct
```yaml
ëª¨ë¸ í¬ê¸°: 14B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 8-10GB (4bit ì–‘ìí™”)
ì†ë„: ì¤‘ê°„ (RTX 3060ì—ì„œ 20-30 tokens/s)
í•œêµ­ì–´: ë§¤ìš° ìš°ìˆ˜
íŠ¹ì§•:
  - ë” ë†’ì€ ì •í™•ë„
  - ë³µì¡í•œ ì¶”ë¡  ëŠ¥ë ¥
  - ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆì„ ë•Œ ì¶”ì²œ
```

### 2.2 ğŸ¥ˆ 2ìˆœìœ„: Llama 3.2 ì‹œë¦¬ì¦ˆ

#### Llama-3.2-3B-Instruct
```yaml
ëª¨ë¸ í¬ê¸°: 3B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 2-3GB
ì†ë„: ë§¤ìš° ë¹ ë¦„ (CPUì—ì„œë„ ì‹¤ìš©ì )
í•œêµ­ì–´: ë³´í†µ
íŠ¹ì§•:
  - ê°€ë²¼ìš´ ëª¨ë¸
  - ë¹ ë¥¸ ì‘ë‹µ
  - í•œêµ­ì–´ëŠ” ì¶”ê°€ í•™ìŠµ í•„ìš”
```

#### Llama-3.2-8B-Instruct
```yaml
ëª¨ë¸ í¬ê¸°: 8B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 5-6GB (4bit ì–‘ìí™”)
ì†ë„: ë¹ ë¦„
í•œêµ­ì–´: ë³´í†µ-ì–‘í˜¸
íŠ¹ì§•:
  - Metaì˜ ìµœì‹  ëª¨ë¸
  - ì¢‹ì€ ì¶”ë¡  ëŠ¥ë ¥
  - ì»¤ë®¤ë‹ˆí‹° ì§€ì› í™œë°œ
```

### 2.3 ğŸ¥‰ 3ìˆœìœ„: Mistral ì‹œë¦¬ì¦ˆ

#### Mistral-7B-Instruct-v0.3
```yaml
ëª¨ë¸ í¬ê¸°: 7B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 4-6GB (4bit ì–‘ìí™”)
ì†ë„: ë¹ ë¦„
í•œêµ­ì–´: ë³´í†µ
íŠ¹ì§•:
  - ì•ˆì •ì ì¸ ì„±ëŠ¥
  - íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
  - ì½”ë“œ ì´í•´ ìš°ìˆ˜
```

### 2.4 íŠ¹ìˆ˜ ëª©ì  ëª¨ë¸

#### Solar-10.7B (Upstage)
```yaml
ëª¨ë¸ í¬ê¸°: 10.7B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 6-8GB (4bit ì–‘ìí™”)
ì†ë„: ì¤‘ê°„
í•œêµ­ì–´: ë§¤ìš° ìš°ìˆ˜
íŠ¹ì§•:
  - í•œêµ­ ê¸°ì—… ê°œë°œ
  - í•œêµ­ì–´ íŠ¹í™”
  - ê¹Šì€ ë³‘í•©(depth upscaling) ê¸°ìˆ 
```

#### EEVE-Korean-10.8B
```yaml
ëª¨ë¸ í¬ê¸°: 10.8B íŒŒë¼ë¯¸í„°
ë©”ëª¨ë¦¬ ìš”êµ¬: 6-8GB (4bit ì–‘ìí™”)
ì†ë„: ì¤‘ê°„
í•œêµ­ì–´: ìµœìš°ìˆ˜
íŠ¹ì§•:
  - í•œêµ­ì–´ ì „ë¬¸
  - í•œêµ­ ë¬¸í™” ì´í•´
  - ë§ì¶¤ë²• ê²€ì‚¬ ìš°ìˆ˜
```

## 3. ì„ë² ë”© ëª¨ë¸

### 3.1 ë‹¤êµ­ì–´ ì„ë² ë”©

#### BGE-M3
```yaml
í¬ê¸°: 568MB
ì°¨ì›: 1024
ì–¸ì–´: 100+ ì–¸ì–´ ì§€ì›
íŠ¹ì§•:
  - í•œêµ­ì–´ ìš°ìˆ˜
  - Dense + Sparse ê²€ìƒ‰
  - ColBERT ì§€ì›
```

#### Multilingual-E5-large
```yaml
í¬ê¸°: 1.1GB
ì°¨ì›: 1024
ì–¸ì–´: 100+ ì–¸ì–´ ì§€ì›
íŠ¹ì§•:
  - MS ê°œë°œ
  - ë†’ì€ ì •í™•ë„
  - ê¸´ ë¬¸ì„œ ì§€ì›
```

### 3.2 í•œêµ­ì–´ íŠ¹í™”

#### KoSimCSE-RoBERTa
```yaml
í¬ê¸°: 440MB
ì°¨ì›: 768
íŠ¹ì§•:
  - í•œêµ­ì–´ ì „ìš©
  - ì˜ë¯¸ ìœ ì‚¬ë„ ìš°ìˆ˜
  - ë¹ ë¥¸ ì†ë„
```

## 4. êµ¬í˜„ ì „ëµ

### 4.1 í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

```python
# ìš©ë„ë³„ ëª¨ë¸ ë¶„ë¦¬
models = {
    "chat": "qwen2.5:7b",          # ëŒ€í™”ìš©
    "classification": "qwen2.5:3b",  # ë¶„ë¥˜ìš© (ë¹ ë¥¸ ëª¨ë¸)
    "embedding": "bge-m3",          # ì„ë² ë”©ìš©
    "summary": "qwen2.5:7b"         # ìš”ì•½ìš©
}
```

### 4.2 Ollama ì„¤ì •

```bash
# Qwen2.5 ì„¤ì¹˜ (ì¶”ì²œ)
ollama pull qwen2.5:7b
ollama pull qwen2.5:7b-instruct-q4_K_M  # ì–‘ìí™” ë²„ì „

# ëŒ€ì•ˆ ëª¨ë¸ë“¤
ollama pull llama3.2:8b
ollama pull mistral:7b-instruct
ollama pull solar:10.7b

# ì„ë² ë”© ëª¨ë¸
ollama pull bge-m3
ollama pull nomic-embed-text
```

### 4.3 ëª¨ë¸ ìµœì í™” ì„¤ì •

```yaml
# ollama ëª¨ë¸ ì„¤ì •
model_config:
  qwen2.5-7b:
    num_gpu: 1
    num_thread: 8
    num_ctx: 8192  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    temperature: 0.7
    top_p: 0.9
    repeat_penalty: 1.1

  optimization:
    quantization: "q4_K_M"  # 4bit ì–‘ìí™”
    flash_attention: true    # Flash Attention 2
    kv_cache: true           # KV ìºì‹œ í™œì„±í™”
```

## 5. ì„±ëŠ¥ ë¹„êµí‘œ

| ëª¨ë¸ | í¬ê¸° | ë©”ëª¨ë¦¬ | ì†ë„ | í•œêµ­ì–´ | ì •í™•ë„ | ì¶”ì²œë„ |
|------|------|--------|------|--------|---------|--------|
| Qwen2.5-7B | 7B | 4-6GB | âš¡âš¡âš¡ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| Qwen2.5-14B | 14B | 8-10GB | âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Llama-3.2-3B | 3B | 2-3GB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | â­â­â­ | â­â­â­ |
| Llama-3.2-8B | 8B | 5-6GB | âš¡âš¡âš¡ | â­â­â­ | â­â­â­â­ | â­â­â­ |
| Mistral-7B | 7B | 4-6GB | âš¡âš¡âš¡ | â­â­ | â­â­â­ | â­â­ |
| Solar-10.7B | 10.7B | 6-8GB | âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| EEVE-Korean | 10.8B | 6-8GB | âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

## 6. êµ¬í˜„ ì˜ˆì‹œ

### 6.1 Ollama Python í´ë¼ì´ì–¸íŠ¸

```python
import ollama
from typing import List, Dict

class LocalLLMService:
    def __init__(self):
        self.chat_model = "qwen2.5:7b"
        self.embed_model = "bge-m3"

    async def chat(self, messages: List[Dict], context: str = None):
        """ëŒ€í™” ìƒì„±"""
        if context:
            messages[0]["content"] = f"Context: {context}\n\n{messages[0]['content']}"

        response = await ollama.chat(
            model=self.chat_model,
            messages=messages,
            options={
                "temperature": 0.7,
                "num_ctx": 8192,
                "num_predict": 512
            }
        )
        return response['message']['content']

    async def extract_memory(self, text: str):
        """ë©”ëª¨ë¦¬ ì¶”ì¶œ"""
        prompt = f"""ë‹¤ìŒ ëŒ€í™”ì—ì„œ ê¸°ì–µí•´ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        ëŒ€í™”: {text}

        í˜•ì‹:
        {{
            "facts": [],
            "preferences": [],
            "events": [],
            "relationships": []
        }}
        """

        response = await ollama.generate(
            model=self.chat_model,
            prompt=prompt,
            format="json"
        )
        return response['response']

    async def classify(self, text: str):
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
        categories = ["personal", "preference", "experience", "knowledge", "relationship", "goal", "health", "work"]

        prompt = f"""í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”: {categories}

        í…ìŠ¤íŠ¸: {text}
        ì¹´í…Œê³ ë¦¬:"""

        response = await ollama.generate(
            model="qwen2.5:3b",  # ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            prompt=prompt
        )
        return response['response'].strip()

    async def embed(self, text: str):
        """ì„ë² ë”© ìƒì„±"""
        response = await ollama.embeddings(
            model=self.embed_model,
            prompt=text
        )
        return response['embedding']
```

### 6.2 ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

```python
class OptimizedLLMService:
    def __init__(self):
        # ëª¨ë¸ í’€ ê´€ë¦¬
        self.model_pool = {
            "heavy": "qwen2.5:7b",     # ë³µì¡í•œ ì‘ì—…
            "light": "qwen2.5:3b",      # ê°„ë‹¨í•œ ì‘ì—…
            "embed": "bge-m3"           # ì„ë² ë”©
        }

    async def smart_process(self, task_type: str, input_text: str):
        """ì‘ì—… ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ"""

        # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë³µì¡ë„ í‰ê°€
        complexity = self._assess_complexity(input_text)

        if task_type == "classification":
            model = self.model_pool["light"]
        elif complexity > 0.7:
            model = self.model_pool["heavy"]
        else:
            model = self.model_pool["light"]

        return await self._run_model(model, input_text)

    def _assess_complexity(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ í‰ê°€"""
        factors = {
            "length": len(text) / 1000,
            "entities": len(set(text.split())) / 100,
            "special_chars": len([c for c in text if not c.isalnum()]) / len(text)
        }
        return min(sum(factors.values()) / len(factors), 1.0)
```

## 7. ìµœì¢… ì¶”ì²œ

### ğŸ¯ í”„ë¡œì íŠ¸ì— ìµœì í™”ëœ êµ¬ì„±

```yaml
Primary Configuration:
  Chat Model: Qwen2.5-7B-Instruct (Q4_K_M)
  Classification Model: Qwen2.5-3B (Q4_K_M)
  Embedding Model: BGE-M3
  Fallback Model: Llama-3.2-3B

Hardware Requirements:
  Minimum:
    - RAM: 16GB
    - Storage: 20GB
    - GPU: Optional (GTX 1060 6GB+)

  Recommended:
    - RAM: 32GB
    - Storage: 50GB
    - GPU: RTX 3060 12GB or better

Performance Expectations:
  - Chat Response: 1-2 seconds
  - Classification: <500ms
  - Embedding: <100ms
  - Memory Search: <200ms
```

### ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen2.5:7b-instruct-q4_K_M
ollama pull qwen2.5:3b-q4_K_M
ollama pull bge-m3

# ì„ íƒì  ëª¨ë¸
# ollama pull solar:10.7b  # í•œêµ­ì–´ ê°•í™”
# ollama pull llama3.2:3b  # ë°±ì—…ìš©

echo "ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!"
ollama list
```

## 8. ë‹¤ìŒ ë‹¨ê³„

1. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
3. í•œêµ­ì–´ ì²˜ë¦¬ í’ˆì§ˆ í‰ê°€
4. íŒŒì¸íŠœë‹ í•„ìš”ì„± ê²€í† 