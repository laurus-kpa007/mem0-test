# mem0 LTM í”„ë¡œì íŠ¸ ì„¤ì • ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ì‚¬ì „ ìš”êµ¬ì‚¬í•­](#ì‚¬ì „-ìš”êµ¬ì‚¬í•­)
2. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
3. [ìƒì„¸ ì„¤ì •](#ìƒì„¸-ì„¤ì •)
4. [ëª¨ë¸ ê´€ë¦¬](#ëª¨ë¸-ê´€ë¦¬)
5. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸš€ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- **OS**: Windows 10/11, macOS, Linux
- **Python**: 3.9 ì´ìƒ
- **RAM**: ìµœì†Œ 16GB (ê¶Œì¥ 32GB)
- **Storage**: ìµœì†Œ 20GB ì—¬ìœ  ê³µê°„
- **GPU**: ì„ íƒì‚¬í•­ (NVIDIA GPU ê¶Œì¥)

### í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
1. **Python 3.9+**
2. **Ollama**
3. **Git**
4. **Docker** (ì„ íƒì‚¬í•­)

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ í´ë¡  ë° í™˜ê²½ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-username/mem0-test.git
cd mem0-test

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2ë‹¨ê³„: Ollama ì„¤ì¹˜

#### Windows
```bash
# Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
# https://ollama.com/download/windows ì—ì„œ ì„¤ì¹˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
```

#### macOS
```bash
brew install ollama
```

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 3ë‹¨ê³„: Ollama ì„œë¹„ìŠ¤ ì‹œì‘

```bash
# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# ìƒˆ í„°ë¯¸ë„ì—ì„œ ëª¨ë¸ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python setup_models.py
```

### 4ë‹¨ê³„: ëª¨ë¸ ìë™ ì„¤ì •

```bash
# ëª¨ë¸ ìë™ ê°ì§€ ë° ì„¤ì •
python setup_models.py

# ì˜µì…˜ ì„ íƒ:
# 1. ë¹ ë¥¸ ì„¤ì¹˜ (í•„ìˆ˜ ëª¨ë¸ë§Œ) - ì¶”ì²œ
# 2. ëŒ€í™”í˜• ì„¤ì¹˜ (ì¶”ì²œ ëª¨ë¸ ì„ íƒ)
# 3. ê²€ì¦ë§Œ ìˆ˜í–‰
```

## ğŸ”§ ìƒì„¸ ì„¤ì •

### ì„¤ì • íŒŒì¼ êµ¬ì¡°

í”„ë¡œì íŠ¸ëŠ” ìë™ìœ¼ë¡œ Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ì„ ê°ì§€í•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤.

#### `config/settings.py` ì£¼ìš” í´ë˜ìŠ¤

```python
# ëª¨ë¸ ì„¤ì •
ModelConfig:
  - chat_model: ëŒ€í™”ìš© ëª¨ë¸ (ìë™ ê°ì§€)
  - classification_model: ë¶„ë¥˜ìš© ëª¨ë¸ (ìë™ ê°ì§€)
  - embedding_model: ì„ë² ë”© ëª¨ë¸ (nomic-embed-text)
  - summary_model: ìš”ì•½ìš© ëª¨ë¸
  - fallback_model: ë°±ì—… ëª¨ë¸

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DatabaseConfig:
  - vector_db_type: "qdrant" (ë²¡í„° DB)
  - metadata_db_type: "sqlite" (ë©”íƒ€ë°ì´í„°)
  - redis ìºì‹œ (ì„ íƒì‚¬í•­)

# ë©”ëª¨ë¦¬ ì„¤ì •
MemoryConfig:
  - max_short_term_memories: 100
  - max_long_term_memories: 10000
  - similarity_threshold: 0.7
```

### OllamaManager ê¸°ëŠ¥

```python
from config.settings import OllamaManager, initialize_config

# Ollama ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™”
ollama = OllamaManager()

# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
models = ollama.list_models()
for model in models:
    print(f"{model.name} ({model.size})")

# íŠ¹ì • ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
if ollama.is_model_available("qwen2.5:7b"):
    print("ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama.pull_model("qwen2.5:7b")

# ìë™ ëª¨ë¸ ì„ íƒ
config = initialize_config()  # ìë™ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
```

## ğŸ“¦ ëª¨ë¸ ê´€ë¦¬

### ì¶”ì²œ ëª¨ë¸ ìš°ì„ ìˆœìœ„

#### ëŒ€í™” ëª¨ë¸ (Chat)
1. **qwen2.5:7b** â­â­â­â­â­ - ìµœê³  ì¶”ì²œ
2. **qwen2.5:14b** â­â­â­â­ - ê³ ì„±ëŠ¥
3. **llama3.2:8b** â­â­â­ - ëŒ€ì•ˆ
4. **mistral:7b** â­â­ - ê²½ëŸ‰

#### ë¶„ë¥˜ ëª¨ë¸ (Classification)
1. **qwen2.5:3b** â­â­â­â­â­ - ë¹ ë¥¸ ì²˜ë¦¬
2. **llama3.2:3b** â­â­â­ - ëŒ€ì•ˆ
3. **phi3:3.8b** â­â­â­ - MS ëª¨ë¸

#### ì„ë² ë”© ëª¨ë¸ (Embedding)
1. **nomic-embed-text** â­â­â­â­â­ - ì¶”ì²œ
2. **mxbai-embed-large** â­â­â­â­ - ëŒ€ì•ˆ
3. **bge-large** â­â­â­ - BAAI

### ëª¨ë¸ ìˆ˜ë™ ì„¤ì¹˜

```bash
# í•„ìˆ˜ ëª¨ë¸ ì„¤ì¹˜
ollama pull qwen2.5:7b
ollama pull nomic-embed-text

# ì„ íƒ ëª¨ë¸ ì„¤ì¹˜
ollama pull qwen2.5:3b  # ë¶„ë¥˜ìš©
ollama pull llama3.2:3b  # ë°±ì—…ìš©

# ì„¤ì¹˜ í™•ì¸
ollama list
```

### ëª¨ë¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
# config/config.json ì§ì ‘ í¸ì§‘
{
  "models": {
    "chat_model": "qwen2.5:7b",
    "classification_model": "qwen2.5:3b",
    "embedding_model": "nomic-embed-text",
    "model_params": {
      "chat": {
        "temperature": 0.7,
        "num_ctx": 8192,
        "num_predict": 1024
      }
    }
  }
}
```

## ğŸ³ Docker ì„¤ì • (ì„ íƒì‚¬í•­)

### Qdrant ë²¡í„° DB ì‹¤í–‰

```bash
# Qdrant ì‹¤í–‰
docker run -p 6333:6333 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# Redis ìºì‹œ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
docker run -d -p 6379:6379 redis:alpine
```

### Docker Compose ì‚¬ìš©

```yaml
# docker-compose.yml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_storage:/qdrant/storage

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

```bash
docker-compose up -d
```

## ğŸ§ª ì„¤ì • ê²€ì¦

### 1. Python í™˜ê²½ í™•ì¸

```bash
python -c "import sys; print(f'Python {sys.version}')"
python -c "import ollama; print('Ollama íŒ¨í‚¤ì§€ OK')"
python -c "import mem0; print('mem0 íŒ¨í‚¤ì§€ OK')"
```

### 2. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸

```python
# test_ollama.py
import ollama

# Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    response = ollama.list()
    print("âœ… Ollama ì—°ê²° ì„±ê³µ")
    print(f"ì„¤ì¹˜ëœ ëª¨ë¸: {[m['name'] for m in response['models']]}")
except Exception as e:
    print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
```

### 3. ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

```python
# test_system.py
from config.settings import initialize_config

# ì„¤ì • ì´ˆê¸°í™” ë° ê²€ì¦
config = initialize_config()

print("=== ì‹œìŠ¤í…œ ì„¤ì • ===")
print(f"ëŒ€í™” ëª¨ë¸: {config.models.chat_model}")
print(f"ì„ë² ë”© ëª¨ë¸: {config.models.embedding_model}")
print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {config.data_dir}")
print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
```

## â— ë¬¸ì œ í•´ê²°

### Ollama ê´€ë ¨ ë¬¸ì œ

#### "Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"
```bash
# ì„¤ì¹˜ í™•ì¸
ollama --version

# ì¬ì„¤ì¹˜ í•„ìš”ì‹œ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
# https://ollama.com/download
```

#### "Ollama ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve

# í¬íŠ¸ í™•ì¸ (ê¸°ë³¸ 11434)
netstat -an | grep 11434
```

#### "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
```bash
# ëª¨ë¸ ì¬ì„¤ì¹˜
ollama pull qwen2.5:7b

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list
```

### Python íŒ¨í‚¤ì§€ ë¬¸ì œ

#### "ModuleNotFoundError"
```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
which python  # Mac/Linux
where python  # Windows

# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install -r requirements.txt
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
config.models.chat_model = "qwen2.5:3b"  # 7b ëŒ€ì‹ 
config.models.classification_model = "qwen2.5:1.5b"  # 3b ëŒ€ì‹ 
```

#### RAM ë¶€ì¡±
```bash
# ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©
ollama pull qwen2.5:7b-q4_0  # 4bit ì–‘ìí™”
```

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ì§€ì†ë˜ë©´:
1. GitHub Issuesì— ë¬¸ì œ ë³´ê³ 
2. ë¡œê·¸ íŒŒì¼ í™•ì¸: `logs/` ë””ë ‰í† ë¦¬
3. ì„¤ì • íŒŒì¼ í™•ì¸: `config/config.json`

## ë‹¤ìŒ ë‹¨ê³„

ì„¤ì •ì´ ì™„ë£Œë˜ë©´:
1. `python main.py` ì‹¤í–‰í•˜ì—¬ ì„œë¹„ìŠ¤ ì‹œì‘
2. `http://localhost:8000` ì ‘ì†
3. API ë¬¸ì„œ: `http://localhost:8000/docs`